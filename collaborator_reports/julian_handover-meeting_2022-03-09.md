Julian handover meeting 

Most of the Code i
https://gitlab.inf.uni-konstanz.de/julian.jandeleit/ushichka-registration

* 'main_' files are the important ones 
* all helper functions are in the same .m file

* 'x*' files show experiments or demos

## main-01_format_raw.m

* All raw video data is read in, camera calibration data read out and re-written into separate data structures

## main_01_mesh_to_dm.ipynb

* generates a depth map - the projection matrix was manually generated by moving a camera through the LiDAR space and copying the values ina software

## main_02_pdmcp.mlx

* Find corresponding 2D points between thermal camera view and depth map
* Move the 2D points from thermal space closer and closer 
* There is no need for triangulation
* This is a 'pose estimation' you know the 3D points from the depth map - you need to figure out the camera centres and orientations
* JJ actually uses the DLT algorithm to figure out where one camera is using the multiple 2D correspondences that we have.
* scale_factor between the camera calibration and LiDAR is ~1.6 -- wonder where this comes from. 
* Having found the position of ONE camera in the LiDAR, then we can 'move' the 3D points  from LiDAR to camera calibration space (compare the projection matrices of the same camera in lidar and calibration space)
* Move the depth-map 3D points into camera calibration space 
* JJ *did not* undistort the 2D points - it wasn't worth the effort (human error in correspondence point matching was bigger he suspects). 

## main_03_backproject_estimated.ipynb

* Validates and visualises the results ( the cameras placed in the LiDAR space)
* Allows you to check the validity of the final alignment 

## startup.m
Includes all functions required to run MATLAB code. 


## automatic_* series
The automatic feature matching experiments in the beginning of the thesis work 

## h_backproject_mesh.py
* Converts pinhole to computer graphics camera model 
* Also generates th depth map after performing the data format conversions

## manual_*
Predecessor the DMCP algorithms.

## mw* 
Experiments with 3D microphone points as correspondence inputs 

## x_camera_pose_from_ushichka_dlt.m
How to convert the camera intrinsics/extrinsics, orientation etc.

## x_compute_descriptors
Compares phase congruency descriptor with SIFT 

## x_cut_lidar_to_roi.ipynb
Cuts out parts of the cave LiDAR to the small region of interest 

## x_demo_fundmat.mlx
Attempts at reconstructing the scene and estimating the fundamental matrix

## x_demo_pose_from_dnn_corresp.mlx
First steps at the DMCP algorithm - prob'ly doesn't work 

## x_manual_back_reconstruction.mlx
Attempts with the depth map reconstruction - prob'ly not useful 

## x_match_epipolar_lghd.mlx
Attempts at using phase congruency and epipolar line matching - v few matches. 

## x_projection_matrices.mlx
Possibly a good reference to refer to all the matrices. Conversions from extrinsic-> projection, camera matrix etc etc. Has all of the conversion in them. A good reference 

## x_remove_fpn_stripes.mlx
Removing the fixed pattern noise. The fixed pattern noise  caused a lot of issues with the initial automatic feature matching experiments.

Removed all horizontal and vertical components of the 2D FFT, to get rid of the fixed pattern noise 

## x_robust_matching.mlx
More phase congruency matching experiments 

## absoluteOrientationHornQuarternion/
* Library code to estimate quarternions

## data/
* Results of experiments in the subdirectories 
	* formatted : after *format_raw*
	* dmcp : results of the main_02_pdmcp
	* mw_reconstruction: mic-wall reconstruction experiments
	* raw : original input data shared with JJ
## Other things that aren't in the Bachelor's thesis
* JJ was actually working on voxel based reconstructions (algorithm called space carving/coolouring?) in the past few months before the submission deadline - he thinks this approach may actually provide a better estimation of the cameras' position. 
* Finding 2D point correspondences was difficult using 'normal' ways 
* Given a 3D volume - multiple cameras looking at the same point should 'show' the same 'colour' for that point. Points that have the same colour on all cameras can be used to do 3D reconstruction 
* JJ thinks this approach is ideally the best way to move forward - but the thermal data is so uniform often that it's tricky to figure out where the correspondences are. 
* meshCat - package 
* JJ also used a phase congruency approach

## Dependencies to include manually 
* `vlfeat` - a folder with all of the code needs to be part of the main directory

## Other notes
* One way to check the alignment results from JJ's current work is to transform the 3D microphone xyz to LiDAR frame - if everything is aligned fine - all the mic points should be on/close to the LiDAR surface
* ICP could improve the results of the LiDAR-camera xyz alignment 